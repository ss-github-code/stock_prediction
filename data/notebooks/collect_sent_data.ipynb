{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecbc983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLEEP_DURATION = 7\n",
    "\n",
    "def find_news_articles(begindate, nytimes_section):\n",
    "    base_url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?'\n",
    "    api_key = '&api-key=INSERT_KEY_HERE'\n",
    "    facet_str = f'&facet=true&begin_date={begindate}&end_date={begindate}'\n",
    "    \n",
    "    page = 0\n",
    "    count = 0\n",
    "    \n",
    "    ret_list = []\n",
    "    while True:\n",
    "        url = base_url+nytimes_section+facet_str+f'&page={page}'+api_key\n",
    "        r = requests.get(url)\n",
    "        if r.status_code != 200:\n",
    "            print(r.status_code)\n",
    "        data = json.loads(r.content)\n",
    "        time.sleep(SLEEP_DURATION)\n",
    "        if page == 0:\n",
    "            tot_articles = data['response']['meta']['hits']\n",
    "            print(begindate, nytimes_section, 'tot_articles', tot_articles)\n",
    "        for i, doc in enumerate(data['response']['docs']):\n",
    "            ret_list.append((doc['headline']['main'], doc['lead_paragraph'], doc['web_url']))\n",
    "            count += 1\n",
    "        if count >= tot_articles:\n",
    "            break\n",
    "        page += 1\n",
    "    #print(len(ret_list))\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d499cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "\n",
    "today = datetime.datetime.today()\n",
    "begin_date = datetime.datetime.strptime('25/03/2007', '%d/%m/%Y')\n",
    "\n",
    "while begin_date <= today:\n",
    "\n",
    "    daily_sentiment = defaultdict(defaultdict)\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    i = 0\n",
    "    while i < BATCH_SIZE:\n",
    "        days_sentiment_pos, days_sentiment_neg, days_sentiment_neu, days_sentiment_comp = 0, 0, 0, 0\n",
    "        date_str = str(begin_date.year) + str(begin_date.month).zfill(2) + str(begin_date.day).zfill(2)\n",
    "\n",
    "        news_desk_str = 'fq=news_desk:(\"Financial\" \"Business\" \"Business Day\")'\n",
    "        section_str = 'fq=section_name:(\"Your Money\" \"Business\" \"Business Day\")'\n",
    "\n",
    "        news_desk_list = find_news_articles(date_str, news_desk_str)\n",
    "        section_list = find_news_articles(date_str, section_str)\n",
    "\n",
    "        final_urls = set()\n",
    "        for news in news_desk_list: # tuple of 3: headline, lead_paragraph, web_url\n",
    "            if news[2] not in final_urls:\n",
    "                # print('adding news desk article', news[0])\n",
    "                final_urls.add(news[2])\n",
    "                sentiment_dict = sid_obj.polarity_scores(news[0] + news[1])\n",
    "                days_sentiment_pos += sentiment_dict['pos']\n",
    "                days_sentiment_neg += sentiment_dict['neg']\n",
    "                days_sentiment_neu += sentiment_dict['neu']\n",
    "                days_sentiment_comp += sentiment_dict['compound']\n",
    "        for news in section_list: # tuple of 3: headline, lead_paragraph, web_url\n",
    "            if news[2] not in final_urls:\n",
    "                # print('adding section article', news[0])\n",
    "                final_urls.add(news[2])\n",
    "                sentiment_dict = sid_obj.polarity_scores(news[0] + news[1])\n",
    "                days_sentiment_pos += sentiment_dict['pos']\n",
    "                days_sentiment_neg += sentiment_dict['neg']\n",
    "                days_sentiment_neu += sentiment_dict['neu']\n",
    "                days_sentiment_comp += sentiment_dict['compound']\n",
    "\n",
    "        num_news_items = len(final_urls)\n",
    "        if num_news_items > 0:\n",
    "            daily_sentiment[date_str]['pos'] = days_sentiment_pos/num_news_items\n",
    "            daily_sentiment[date_str]['neg'] = days_sentiment_neg/num_news_items\n",
    "            daily_sentiment[date_str]['neu'] = days_sentiment_neu/num_news_items\n",
    "            daily_sentiment[date_str]['compound'] = days_sentiment_comp/num_news_items\n",
    "        else:\n",
    "            daily_sentiment[date_str]['pos'] = 0\n",
    "            daily_sentiment[date_str]['neg'] = 0\n",
    "            daily_sentiment[date_str]['neu'] = 0\n",
    "            daily_sentiment[date_str]['compound'] = 0\n",
    "        i += 1\n",
    "        begin_date += datetime.timedelta(days=1)\n",
    "    df = pd.DataFrame(daily_sentiment).T\n",
    "    df.to_csv('sentiment_data/' + date_str + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
